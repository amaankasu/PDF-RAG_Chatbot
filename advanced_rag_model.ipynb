{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv==0.21.1 (from -r requirements.txt (line 1))\n",
      "  Using cached python_dotenv-0.21.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting llama-index==0.9.40 (from -r requirements.txt (line 2))\n",
      "  Using cached llama_index-0.9.40-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting PyMuPDF==1.23.1 (from -r requirements.txt (line 3))\n",
      "  Using cached PyMuPDF-1.23.1-cp311-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting spacy==3.5.0 (from -r requirements.txt (line 4))\n",
      "  Using cached spacy-3.5.0-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Collecting numpy==1.23.5 (from -r requirements.txt (line 5))\n",
      "  Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting faiss-cpu==1.7.3 (from -r requirements.txt (line 6))\n",
      "  Using cached faiss_cpu-1.7.3-cp311-cp311-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting sentence-transformers==2.3.1 (from -r requirements.txt (line 7))\n",
      "  Using cached sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: openai==0.28.0 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.28.0)\n",
      "Requirement already satisfied: rank_bm25==0.2.2 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from -r requirements.txt (line 9)) (0.2.2)\n",
      "Collecting pydantic==1.10.14 (from -r requirements.txt (line 10))\n",
      "  Using cached pydantic-1.10.14-cp311-cp311-win_amd64.whl.metadata (151 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.40->-r requirements.txt (line 2)) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (3.10.11)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (2025.2.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\amaan\\desktop\\rag chatbot\\advanced_rag_venv\\lib\\site-packages (from llama-index==0.9.40->-r requirements.txt (line 2)) (3.9.1)\n",
      "INFO: pip is looking at multiple versions of llama-index to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested openai==0.28.0\n",
      "    llama-index 0.9.40 depends on openai>=1.1.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install -r requirements.txt (line 2) and openai==0.28.0 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/12.8 MB 525.1 kB/s eta 0:00:25\n",
      "     - -------------------------------------- 0.6/12.8 MB 4.8 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 8.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 9.5 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 10.8 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.0/12.8 MB 11.0 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.6/12.8 MB 11.4 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 11.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.9/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.5/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 12.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.3/12.8 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.0/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.6/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.2/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 13.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amaan\\Desktop\\RAG Chatbot\\advanced_rag_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging (you can adjust logging level for debugging)\n",
    "logging.basicConfig(\n",
    "    filename=\"benchmark.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Key: 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\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables (ensure you have a .env file with OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "print(\"Loaded API Key:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Load spaCy model and adjust max_length for long PDFs\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# Load Sentence Transformer model for embeddings\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# In-memory dictionary to store processed document data\n",
    "documents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING ALL THE REQUIRED FUNCTIONS\n",
    "\n",
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Fallback extraction using PyMuPDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_structured_content(file_path: str):\n",
    "    \"\"\"\n",
    "    Uses Unstructured to partition a PDF into structured elements.\n",
    "    Returns a list of dictionaries with keys \"type\" and \"text\".\n",
    "    \"\"\"\n",
    "    elements = partition_pdf(filename=file_path)\n",
    "    structured_data = []\n",
    "    for element in elements:\n",
    "        structured_data.append({\n",
    "            \"type\": element.type,  # e.g., \"Title\", \"Heading\", \"Text\"\n",
    "            \"text\": element.text.strip(),\n",
    "        })\n",
    "    return structured_data\n",
    "\n",
    "def tag_sections_technical(structured_elements):\n",
    "    \"\"\"\n",
    "    Groups extracted elements into sections for technical papers.\n",
    "    This regex captures common technical section headers including:\n",
    "      Abstract, Introduction, Related Work, Background, Methodology,\n",
    "      Approach, Experiments, Results, Discussion, Conclusion,\n",
    "      Encoding, CLIP, Text Encoder, Embedding.\n",
    "    \"\"\"\n",
    "    section_pattern = re.compile(\n",
    "        r\"(Abstract|Introduction|Related Work|Background|Methodology|Approach|Experiments|Results|Discussion|Conclusion|Encoding|CLIP|Text Encoder|Embedding)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    tagged_sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    for element in structured_elements:\n",
    "        element_type = element.get(\"type\", \"\").lower()\n",
    "        text = element.get(\"text\", \"\")\n",
    "        if element_type in [\"heading\", \"title\"] or section_pattern.search(text):\n",
    "            match = section_pattern.search(text)\n",
    "            new_section = match.group(0).strip() if match else text.strip()\n",
    "            current_section = new_section\n",
    "            if current_section not in tagged_sections:\n",
    "                tagged_sections[current_section] = []\n",
    "        elif current_section:\n",
    "            tagged_sections[current_section].append(text)\n",
    "        else:\n",
    "            tagged_sections.setdefault(\"Body\", []).append(text)\n",
    "    \n",
    "    for section in tagged_sections:\n",
    "        tagged_sections[section] = \"\\n\".join(tagged_sections[section]).strip()\n",
    "    return tagged_sections\n",
    "\n",
    "def robust_extract_text(file_path: str) -> (str, dict):\n",
    "    \"\"\"\n",
    "    Attempts to extract text via structured partitioning and tag technical sections.\n",
    "    Falls back to basic extraction if necessary.\n",
    "    Returns (combined_text, tagged_sections).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        structured_elements = extract_structured_content(file_path)\n",
    "        tagged_sections = tag_sections_technical(structured_elements)\n",
    "        combined_text = \"\\n\\n\".join([f\"{section}: {content}\" for section, content in tagged_sections.items()])\n",
    "        if combined_text.strip():\n",
    "            return combined_text, tagged_sections\n",
    "        else:\n",
    "            raise Exception(\"No structured content extracted.\")\n",
    "    except Exception as e:\n",
    "        logging.info(\"Structured extraction failed; using fallback extraction. Error: \" + str(e))\n",
    "        fallback_text = extract_text_from_pdf(file_path)\n",
    "        return fallback_text, {}\n",
    "\n",
    "def adaptive_chunk_text_dynamic(text: str, min_threshold: int = None, factor: float = 1.5, transition_words=None) -> list:\n",
    "    \"\"\"\n",
    "    Splits text into semantically coherent chunks using a dynamic token threshold\n",
    "    based on the average sentence length. Also uses linguistic cues (transition words)\n",
    "    to determine natural boundaries between clauses.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): Input text to chunk.\n",
    "      min_threshold (int): Optional minimum token threshold. If None, defaults to the average tokens.\n",
    "      factor (float): Multiplier for the average token count to set the threshold.\n",
    "      transition_words (list): List of transition words indicating a potential chunk boundary.\n",
    "      \n",
    "    Returns:\n",
    "      list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Clean up whitespace in the text.\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Use spaCy for sentence segmentation.\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    # Calculate average token count per sentence.\n",
    "    token_counts = [len(sent.split()) for sent in sentences]\n",
    "    if not token_counts:\n",
    "        return [text]\n",
    "    avg_tokens = sum(token_counts) / len(token_counts)\n",
    "    \n",
    "    # Set dynamic threshold: maximum of a given min_threshold or a factor of the average.\n",
    "    if min_threshold is None:\n",
    "        min_threshold = int(avg_tokens)\n",
    "    threshold = int(max(min_threshold, avg_tokens * factor))\n",
    "    \n",
    "    # Default transition words if not provided.\n",
    "    if transition_words is None:\n",
    "        transition_words = [\"however\", \"moreover\", \"furthermore\", \"in conclusion\", \"finally\", \"additionally\"]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_token_count = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent_tokens = len(sent.split())\n",
    "        \n",
    "        # Check if current chunk plus this sentence would exceed the dynamic threshold.\n",
    "        # Also, if the sentence starts with a transition word and the current chunk is already near the threshold,\n",
    "        # consider that as a natural boundary.\n",
    "        if current_chunk:\n",
    "            sent_lower = sent.lower()\n",
    "            starts_with_transition = any(sent_lower.startswith(word) for word in transition_words)\n",
    "        else:\n",
    "            starts_with_transition = False\n",
    "        \n",
    "        # Determine if we should break the chunk.\n",
    "        if (current_token_count + sent_tokens > threshold) or (starts_with_transition and current_token_count > int(threshold * 0.7)):\n",
    "            # Save the current chunk and reset.\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent\n",
    "            current_token_count = sent_tokens\n",
    "        else:\n",
    "            current_chunk += \" \" + sent\n",
    "            current_token_count += sent_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_embeddings(chunks: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes embeddings for each text chunk using the Sentence Transformer.\n",
    "    \"\"\"\n",
    "    embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
    "    return embeddings.astype(\"float32\")\n",
    "\n",
    "def build_hnsw_index(embeddings: np.ndarray, M: int = 32, efConstruction: int = 40):\n",
    "    \"\"\"\n",
    "    Builds a FAISS HNSW index from the computed embeddings.\n",
    "    \"\"\"\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexHNSWFlat(d, M)\n",
    "    index.hnsw.efConstruction = efConstruction\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def search_index(query: str, index, chunks: list, k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Uses vector search to retrieve the top-k candidate chunks.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    query_embedding = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "    search_duration = time.time() - start_time\n",
    "    logging.info(f\"HNSW Search Time: {search_duration:.4f} seconds\")\n",
    "    return results\n",
    "\n",
    "def bm25_filter(query, candidate_chunks, threshold=1.0):\n",
    "    if not candidate_chunks:  # Check for empty input\n",
    "        print(\"BM25 received an empty candidate chunk list.\")\n",
    "        return []\n",
    "    \n",
    "    tokenized_corpus = [doc.lower().split() for doc in candidate_chunks if doc.strip()]  # Remove empty docs\n",
    "    if not tokenized_corpus:  # Ensure there are valid documents\n",
    "        print(\"BM25 corpus is empty after processing. Returning no results.\")\n",
    "        return []\n",
    "    \n",
    "    bm25 = BM25Okapi(tokenized_corpus)  # Now safe to initialize\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    # Get the highest-ranked document(s)\n",
    "    best_idx = np.argmax(scores)\n",
    "    if scores[best_idx] >= threshold:\n",
    "        return [candidate_chunks[best_idx]]\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_answer(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer using OpenAI's ChatCompletion API based on the provided context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are given the following context extracted from a technical paper:\n",
    "    \n",
    "{context}\n",
    "\n",
    "Based on the above context, answer the following question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-3.5-turbo\" as appropriate\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        answer = f\"Error generating answer: {e}\"\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(chunks: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes embeddings for each text chunk using the Sentence Transformer.\n",
    "    \"\"\"\n",
    "    embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
    "    return embeddings.astype(\"float32\")\n",
    "\n",
    "def build_hnsw_index(embeddings: np.ndarray, M: int = 32, efConstruction: int = 40):\n",
    "    \"\"\"\n",
    "    Builds a FAISS HNSW index from the computed embeddings.\n",
    "    \"\"\"\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexHNSWFlat(d, M)\n",
    "    index.hnsw.efConstruction = efConstruction\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def search_index(query: str, index, chunks: list, k: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Uses vector search to retrieve the top-k candidate chunks.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    query_embedding = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "    search_duration = time.time() - start_time\n",
    "    logging.info(f\"HNSW Search Time: {search_duration:.4f} seconds\")\n",
    "    return results\n",
    "\n",
    "def bm25_filter(query, candidate_chunks, threshold=1.0):\n",
    "    if not candidate_chunks:  # Check for empty input\n",
    "        print(\"BM25 received an empty candidate chunk list.\")\n",
    "        return []\n",
    "    \n",
    "    tokenized_corpus = [doc.lower().split() for doc in candidate_chunks if doc.strip()]  # Remove empty docs\n",
    "    if not tokenized_corpus:  # Ensure there are valid documents\n",
    "        print(\"BM25 corpus is empty after processing. Returning no results.\")\n",
    "        return []\n",
    "    \n",
    "    bm25 = BM25Okapi(tokenized_corpus)  # Now safe to initialize\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    # Get the highest-ranked document(s)\n",
    "    best_idx = np.argmax(scores)\n",
    "    if scores[best_idx] >= threshold:\n",
    "        return [candidate_chunks[best_idx]]\n",
    "    return []\n",
    "\n",
    "\n",
    "def generate_answer(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer using OpenAI's ChatCompletion API based on the provided context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are given the following context extracted from a technical paper:\n",
    "    \n",
    "{context}\n",
    "\n",
    "Based on the above context, answer the following question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-3.5-turbo\" as appropriate\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        answer = f\"Error generating answer: {e}\"\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Sections:\n",
      "Total Chunks: 91\n",
      "Sample Chunk (first 300 characters):\n",
      "STANDARD LEASE AGREEMENT THIS LEASE AGREEMENT hereinafter known as the \"Lease\" is made and entered into this ____ ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving candidate chunks for query: 'Shall Landlord be liable for any injury to the tenant?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 132.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW Search Time: 0.0115 seconds\n",
      "\n",
      "Candidate Chunks and Similarity Scores:\n",
      "Chunk 1:\n",
      "Similarity Score: 0.471958726644516\n",
      "Text (first 300 characters): If said damage was due to the negligence of the Tenant(s), the Tenant(s) shall be liable to the Landlord for all repairs and for the loss of income due to restoring the property back to a livable condition in addition to any other losses that can be proved by the Landlord....\n",
      "\n",
      "Chunk 2:\n",
      "Similarity Score: 0.5371958017349243\n",
      "Text (first 300 characters): INDEMNIFICATION.â€‹ Landlord shall not be liable for any injury to the tenant, tenantâ€™s family, guests, or employees or to any person entering the Property and shall not be liable for any damage to the building in which the Property is located or to goods or equipment, or to the structure or equipment...\n",
      "\n",
      "Chunk 3:\n",
      "Similarity Score: 0.6724002361297607\n",
      "Text (first 300 characters): Tenant is responsible and liable for any damage or required cleaning to the Property caused by any authorized or unauthorized animal and for all costs Landlord may incur in removing or causing any animal to be removed....\n",
      "\n",
      "Chunk 4:\n",
      "Similarity Score: 0.6862563490867615\n",
      "Text (first 300 characters): DEFAULT.â€‹ If Landlord breaches this Lease, Tenant may seek any relief provided by law....\n",
      "\n",
      "BM25 Filtered Candidate Found.\n",
      "\n",
      "Final Context for LLM Prompt:\n",
      "INDEMNIFICATION.â€‹ Landlord shall not be liable for any injury to the tenant, tenantâ€™s family, guests, or employees or to any person entering the Property and shall not be liable for any damage to the building in which the Property is located or to goods or equipment, or to the structure or equipment of the structure in which the Property is located, and Tenant hereby agrees to indemnify, defend, and hold Landlord harmless from any and all claims or assertions of every kind and nature. \n",
      "\n",
      "Generated Answer:\n",
      "No, the Landlord shall not be liable for any injury to the tenant.\n"
     ]
    }
   ],
   "source": [
    "# Example: Process a PDF and run a test query.\n",
    "\n",
    "test_pdf_path = r\"C:\\Users\\amaan\\Desktop\\RAG Chatbot\\data\\standard-residential-lease-agreement-template.pdf\"\n",
    "\n",
    "# Step 1: Extract and tag text from the PDF.\n",
    "extracted_text, tagged_sections = robust_extract_text(test_pdf_path)\n",
    "print(\"Tagged Sections:\")\n",
    "for section, content in tagged_sections.items():\n",
    "    print(f\"--- {section} ---\\n{content[:300]}...\\n\")  # Print first 300 characters for brevity\n",
    "\n",
    "# Step 2: Adaptive Chunking.\n",
    "chunks = adaptive_chunk_text_dynamic(extracted_text)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(\"Sample Chunk (first 300 characters):\")\n",
    "print(chunks[0][:300], \"...\\n\")\n",
    "\n",
    "# Step 3: Generate embeddings and build the FAISS index.\n",
    "embeddings = get_embeddings(chunks)\n",
    "index = build_hnsw_index(embeddings)\n",
    "\n",
    "# Step 4: Retrieve candidate chunks with similarity scores.\n",
    "query = \"Shall Landlord be liable for any injury to the tenant?\"\n",
    "print(f\"Retrieving candidate chunks for query: '{query}'\")\n",
    "start_time = time.time()\n",
    "query_embedding = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "distances, indices = index.search(query_embedding, k=4)\n",
    "search_duration = time.time() - start_time\n",
    "print(f\"HNSW Search Time: {search_duration:.4f} seconds\\n\")\n",
    "\n",
    "candidate_chunks = []\n",
    "print(\"Candidate Chunks and Similarity Scores:\")\n",
    "for i, (idx, score) in enumerate(zip(indices[0], distances[0])):\n",
    "    if idx < len(chunks):\n",
    "         candidate_chunks.append(chunks[idx])\n",
    "         print(f\"Chunk {i+1}:\")\n",
    "         print(f\"Similarity Score: {score}\")\n",
    "         print(f\"Text (first 300 characters): {chunks[idx][:300]}...\\n\")\n",
    "\n",
    "# Step 5: Optionally filter candidate chunks using BM25 for keyword relevance.\n",
    "filtered_chunks = bm25_filter(query, candidate_chunks, threshold=1.0)\n",
    "if filtered_chunks:\n",
    "    final_context = filtered_chunks[0]  # Select the top candidate from BM25 filtering.\n",
    "    print(\"BM25 Filtered Candidate Found.\\n\")\n",
    "else:\n",
    "    final_context = \"\\n\\n\".join(candidate_chunks)\n",
    "    print(\"No BM25 filter candidates passed the threshold. Using all candidate chunks.\\n\")\n",
    "\n",
    "print(\"Final Context for LLM Prompt:\")\n",
    "print(final_context, \"\\n\")\n",
    "\n",
    "# Step 6: Generate an answer using the final context.\n",
    "answer = generate_answer(query, final_context)\n",
    "print(\"Generated Answer:\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
